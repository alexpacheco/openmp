\documentclass[c,mathserif,compress,xcolor=svgnames]{beamer} 
\mode<presentation>
{  
  \setbeamertemplate{background canvas}[vertical shading][bottom=blue!5,top=blue!5]
  \setbeamertemplate{navigation symbols}{}%{\insertsectionnavigationsymbol}
    \usetheme{LSU}
%  default infolines miniframes shadow sidebar smoothbars smoothtree split tree
%    \useoutertheme{shadow}
}

\usefonttheme{professionalfonts}
\usefonttheme{serif}

\usepackage{pgf,pgfarrows,pgfnodes,pgfautomata,pgfheaps,pgfshade}
\usepackage{amsmath,amssymb,amsfonts,subfigure}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{colortbl}
\usepackage{etex}
\usepackage{fancyvrb,listings}
\lstdefinestyle{customc}{
  belowcaptionskip=1\baselineskip,
  breaklines=true,
  xleftmargin=\parindent,
  language=C,
  showstringspaces=false,
  basicstyle=\tiny\ttfamily,
  keywordstyle=\bfseries\color{green!40!black},
  commentstyle=\itshape\color{red!90!white},
  identifierstyle=\color{blue},
  stringstyle=\color{orange},
}
\lstdefinelanguage{OmpFortran}[]{Fortran}{
   rulesepcolor=\color{black},
   %
   extendedchars=true,
   %
   morecomment=[l] [\bfseries\color{red!90!white}]{!\$omp},
   morecomment=[l] [\bfseries\color{red!90!white}]{c\$omp},
   morecomment=[l] [\bfseries\color{red!90!white}]{*\$omp},
}[comments]

\lstdefinelanguage{OmpC}[]{OmpFortran}{
   rulesepcolor=\color{black},
   %
   extendedchars=true,
   %
   morecomment=[l] [\bfseries\color{red!90!white}]{\#pragma\ omp},
   morecomment=[l] [\itshape\color{red!90!white}]{/*},
}[comments]

\lstset{escapechar=@,style=customc}
\lstset{literate=%
   *{0}{{{\color{blue}0}}}1
    {1}{{{\color{blue}1}}}1
    {2}{{{\color{blue}2}}}1
    {3}{{{\color{blue}3}}}1
    {4}{{{\color{blue}4}}}1
    {5}{{{\color{blue}5}}}1
    {6}{{{\color{blue}6}}}1
    {7}{{{\color{blue}7}}}1
    {8}{{{\color{blue}8}}}1
    {9}{{{\color{blue}9}}}1
}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,automata}
\usetikzlibrary{calc,fit,shadows,patterns}
\pgfdeclarelayer{background}
\pgfdeclarelayer{foreground}
\pgfsetlayers{background,main,foreground}
\usepackage[latin1]{inputenc}
\usepackage{colortbl}
\usepackage[english]{babel}
\usepackage{hyperref}
\usepackage[normalem]{ulem}
% \usepackage{movie15}
\hypersetup{
  pdftitle={Introduction to OpenMP},
  pdfauthor={Alexander B. Pacheco, User Services Consultant, Louisiana State University}
}
%\usepackage{movie15}
\usepackage{times}
\setbeamercovered{dynamic}
\beamersetaveragebackground{DarkBlue!2}
\beamertemplateballitem

\usepackage[english]{babel}
\usepackage[latin1]{inputenc}
\usepackage{times}
\usepackage{amsmath}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\definecolor{DarkGreen}{rgb}{0.0,0.3,0.0}
\definecolor{darkgreen}{rgb}{0.0,0.6,0.0}
\definecolor{Blue}{rgb}{0.0,0.0,0.8} 
\definecolor{dodgerblue}{rgb}{0.1,0.1,1.0}
\definecolor{indigo}{rgb}{0.41,0.1,0.0}
\definecolor{seagreen}{rgb}{0.1,1.0,0.1}
\DeclareSymbolFont{extraup}{U}{zavm}{m}{n}
\DeclareMathSymbol{\vardiamond}{\mathalpha}{extraup}{87}
\newcommand*\up{\textcolor{green}{%
  \ensuremath{\blacktriangle}}}
\newcommand*\down{\textcolor{red}{%
  \ensuremath{\blacktriangledown}}}
\newcommand*\const{\textcolor{darkgray}%
  {\textbf{--}}}

\setbeamercolor{uppercol}{fg=white,bg=red!30!black}%
\setbeamercolor{lowercol}{fg=black,bg=red!15!white}%
\setbeamercolor{uppercol1}{fg=white,bg=blue!30!black}%
\setbeamercolor{lowercol1}{fg=black,bg=blue!15!white}%%
\setbeamercolor{uppercol2}{fg=white,bg=green!30!black}%
\setbeamercolor{lowercol2}{fg=black,bg=green!15!white}%
\newenvironment{colorblock}[4]
{
\setbeamercolor{upperblock}{fg=#1,bg=#2}
\setbeamercolor{lowerblock}{fg=#3,bg=#4}
\begin{beamerboxesrounded}[upper=upperblock,lower=lowerblock,shadow=true]}
{\end{beamerboxesrounded}}
\newenvironment{ablock}[0]
{
\begin{beamerboxesrounded}[upper=uppercol,lower=lowercol,shadow=true]}
{\end{beamerboxesrounded}}
\newenvironment{bblock}[0]
{
\begin{beamerboxesrounded}[upper=uppercol1,lower=lowercol1,shadow=true]}
{\end{beamerboxesrounded}}
\newenvironment{eblock}[0]
{
\begin{beamerboxesrounded}[upper=uppercol2,lower=lowercol2,shadow=true]}
{\end{beamerboxesrounded}}

% Fix font size of nested itemize/enumerate
\setbeamerfont{itemize/enumerate body}{}
\setbeamerfont{itemize/enumerate subbody}{size=\scriptsize}
\setbeamerfont{itemize/enumerate subsubbody}{size=\scriptsize}

\title[OpenMP]{Introduction to OpenMP}


\author[Alex Pacheco]{\large{Alexander~B.~Pacheco}}
       
\institute[HPC@LSU - http://www.hpc.lsu.edu] {\inst{}\footnotesize{User Services Consultant\\LSU HPC \& LONI\\sys-help@loni.org}}

\date[{June 2-4, 2014\hspace{2cm}}]{\scriptsize{LONI Parallel Programming Workshop\\Louisiana State University\\Baton Rouge\\June 2-4, 2014}}
     
\subject{Talks}
% This is only inserted into the PDF information catalog. Can be left
% out. 




% If you have a file called "university-logo-filename.xxx", where xxx
% is a graphic format that can be processed by latex or pdflatex,
% resp., then you can add a logo as follows:

% Main Logo on bottom left
\pgfdeclareimage[height=0.55cm]{its-logo}{LONI}
\logo{\pgfuseimage{its-logo}}
% University Logo on top left
\pgfdeclareimage[height=0.55cm]{university-logo}{LSUGeauxPurp}
\tllogo{\pgfuseimage{university-logo}}
% Logo at top right
\pgfdeclareimage[height=0.5cm]{institute-logo}{its-logo}
\trlogo{\pgfuseimage{institute-logo}}
% Logo at bottom right
\pgfdeclareimage[height=0.48cm]{hpc-logo}{cct-logo}
\brlogo{\pgfuseimage{hpc-logo}}


% Delete this, if you do not want the table of contents to pop up at
% the beginning of each subsection:
%  \AtBeginSection[]
%  {
%    \begin{frame}<beamer>
%     \frametitle{\small{Outline}}
%      \small
%      \tableofcontents[currentsection,currentsubsection]
%    \end{frame}
%  }

\begin{document}
\footnotesize

\tikzstyle{every picture}+=[remember picture]
\frame{\titlepage}

%\begin{frame}[label=toc,squeeze]
%  \footnotesize
%  \frametitle{\small{Outline}}
%  \tableofcontents
%\end{frame}


%\part{Introduction}
%\section{Introduction}
%\begin{frame}<0>
%  \frametitle{\small Goals}
%  \large{
%  \begin{block}{}
%    \begin{itemize}
%      \item Acquaint users with the concepts of shared memory parallelism.
%      \item Acquaint users with the basics of programming with OpenMP.
%    \end{itemize}
%  \end{block}
%  }
%\end{frame}

\begin{frame}
  \frametitle{\small Distributed Memory Model}
%  \begin{columns}
%    \column{6cm}
    \begin{itemize}
      \item Each process has its own address space
      \begin{itemize}
        \item Data is local to each process
      \end{itemize}
      \item Data sharing is achieved via explicit message passing
      \item Example
      \begin{itemize}
        \item MPI
      \end{itemize}
    \end{itemize}
%    \column{6cm}
%    \begin{center}
%      \includegraphics[width=8cm]{./distributed}
%    \end{center}
%  \end{columns}
%\end{frame}

%\begin{frame}
    \include{distributed}
\end{frame}

\begin{frame}
  \frametitle{\small Shared Memory Model}
%  \begin{columns}
%    \column{6cm}
    \begin{itemize}
      \item All threads can access the global memory space.
      \item Data sharing achieved via writing to/reading from the same memory location
      \item Example
      \begin{itemize}
        \item OpenMP
        \item Pthreads
      \end{itemize}
    \end{itemize}
%    \column{6cm}
    \include{shared}
%  \end{columns}
\end{frame}


\begin{frame}
  \frametitle{\small Clusters of SMP nodes}
  \begin{itemize}
    \item The shared memory model is most commonly represented by Symmetric Multi-Processing (SMP) systems
    \begin{itemize}
      \item Identical processors
      \item Equal access time to memory
    \end{itemize}
    \item Large shared memory systems are rare, clusters of SMP nodes are popular.
  \end{itemize}
%  \begin{columns}
%    \column{13cm}
%    \begin{center}
%      \includegraphics[width=12cm]{./smp-cluster}
%    \end{center}
%  \end{columns}
%\end{frame}

%\begin{frame}
  \include{smpcluster}
\end{frame}

\begin{frame}
  \frametitle{\small Shared vs Distributed}
%  \begin{columns}
%    \column{5cm}
    \begin{eblock}{Shared Memory}
      \begin{itemize}
        \item Pros
        \begin{itemize}
          \item Global address space is user friendly
          \item Data sharing is fast
        \end{itemize}
        \item Cons
        \begin{itemize}
          \item Lack of scalability
          \item Data conflict issues
        \end{itemize}
      \end{itemize}
    \end{eblock}
%    \column{5cm}
    \begin{eblock}{Distributed Memory}
      \begin{itemize}
        \item Pros
        \begin{itemize}
          \item Memory scalable with number of processors
          \item Easier and cheaper to build
        \end{itemize}
        \item Cons
        \begin{itemize}
          \item Difficult load balancing
          \item Data sharing is slow
        \end{itemize}
      \end{itemize}
    \end{eblock}
%  \end{columns}
\end{frame}

\begin{frame}
  \frametitle{\small Parallelizing Serial Code}
  \begin{eblock}{Compiler Flags for Automatic Parallelization}
    \begin{itemize}
      \item[GCC] -floop-parallelize-all
      \item[Intel] -parallel
      \item[XL] -qsmp=auto
      \item[PGI] -Mconcur=<flags>
    \end{itemize}
  \end{eblock}
  \begin{bblock}{When to consider using OpenMP?}
    \begin{itemize}
      \item The compiler may not be able to do the parallelization
      \begin{enumerate}
        \item A loop is not parallelized
        \begin{itemize}
          \item The data dependency analysis is not able to determine whether it is safe to parallelize or not
        \end{itemize}
        \item The granularity is not high enough
        \begin{itemize}
          \item The compiler lacks information to parallelize at the highest possible level
        \end{itemize}
      \end{enumerate}
    \end{itemize}
  \end{bblock}
\end{frame}

\begin{frame}
  \frametitle{\small OpenMP}
  \begin{bblock}{}
    \begin{itemize}
      \item OpenMP is an Application Program Interface (API) for thread based parallelism; Supports Fortran, C and C++
      \item Uses a fork-join execution model
      \item OpenMP structures are built with program directives, runtime libraries and environment variables
      \item OpenMP has been the industry standard for shared memory programming over the last decade
      \begin{itemize}
        \item Permanent members of the OpenMP Architecture Review Board: AMD, Cray, Fujutsu, HP, IBM, Intel, Microsoft, NEC, PGI, SGI, Sun
      \end{itemize}
      \item OpenMP 3.1 was released in September 2011
    \end{itemize}
  \end{bblock}
\end{frame}

\begin{frame}
  \frametitle{\small Advantages of OpenMP}
  \begin{bblock}{}
    \begin{itemize}
      \item Portability
      \begin{itemize}
        \item Standard among many shared memory platforms
        \item Implemented in major compiler suites
      \end{itemize}
      \item Ease to use 
      \begin{itemize}
        \item Serial programs can be parallelized by adding compiler directives
        \item Allows for incremental parallelization - a serial program evolves into a parallel program by parallelizing different sections incrementally
      \end{itemize}
    \end{itemize}
  \end{bblock}
\end{frame}

\begin{frame}
  \frametitle{\small Fork-Join Execution Model}
  \begin{bblock}{}
    \begin{itemize}
      \item Parallelism is achieved by generating multiple threads that run in parallel
      \begin{itemize}
        \item A fork \tikz[scale=0.3,baseline=-0.3em]{\filldraw[fill=white] (0.0,0.0) circle (0.5);\draw[draw=black] (0.0,0.0) node {F};} is when a single thread is made into multiple, concurrently executing threads
        \item A join \tikz[scale=0.3,baseline=-0.3em]{\filldraw[fill=white] (0.0,0.0) circle (0.5);\draw[draw=black] (0.0,0.0) node {J};} is when the concurrently executing threads synchronize back into a single thread
      \end{itemize}
      \item OpenMP programs essentially consist of a series of forks and joins.
    \end{itemize}
    \vspace{-0.8cm}
    \include{fork-join}
  \end{bblock}
\end{frame}

\begin{frame}
  \frametitle{\small Building Block of OpenMP}
  \begin{eblock}{}
    \begin{itemize}
      \item Program directives
        \begin{itemize}
          \item Syntax
            \begin{itemize}
              \item C/C++: \texttt{\#pragma omp <directive> [clause]}
              \item Fortran: \texttt{!\$omp <directive> [clause]}
            \end{itemize}
          \item Parallel regions
          \item Parallel loops
          \item Synchronization
          \item Data Structure
          \item $\cdots$
        \end{itemize}
      \item Runtime library routines
      \item Environment variables
    \end{itemize}
  \end{eblock}
\end{frame}

\begin{frame}
  \frametitle{\small OpenMP Basic Syntax}
  \begin{itemize}
    \item Fortran: case insensitive
    \begin{itemize}
      \item Add: {\color{green!40!black}use} {\color{blue}omp\_lib} or {\color{green!40!black}include} {\color{orange}"omp\_lib.h"}
      \item Usage: {\color{red!90!black}Sentinel directive [clauses]}
      \item Fortran 77
      \begin{itemize}
        \item {\color{red!90!black}Sentinel} could be: {\color{red!90!black}!\$omp, *\$omp, c\$omp} and must begin in first column
      \end{itemize}
      \item Fortran 90/95/2003
      \begin{itemize}
        \item {\color{red!90!black}Sentinel}: {\color{red!90!black}!\$omp}
      \end{itemize}
      \item End of parallel region is signified by the end sentinel statement: {\color{red!90!black}!\$omp end directive [clauses]}
    \end{itemize}
    \item C/C++: case sensitive
    \begin{itemize}
      \item Add {\color{green!40!black}\#include} <{\color{blue}omp.h}>
      \item Usage: {\color{red!90!black}\#pragma omp directive [clauses] newline}
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{\small Compiler Directives}
  \begin{itemize}
    \item Parallel Directive
    \begin{itemize}
      \item {\bf\color{red!90!black}parallel}
    \end{itemize}
    \item Worksharing Constructs
    \begin{itemize}
      \item Fortran: {\bf\color{red!90!black}do}, {\bf\color{red!90!black}workshare}
      \item C/C++: {\bf\color{red!90!black}for}
      \item Fortran/C/C++: {\bf\color{red!90!black}sections}
    \end{itemize}
    \item Synchronization
    \begin{itemize}
      \item {\bf\color{red!90!black}master}, {\bf\color{red!90!black}single}, {\bf\color{red!90!black}ordered}, {\bf\color{red!90!black}flush}, {\bf\color{red!90!black}atomic}
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{\small Clauses}
  \begin{itemize}
    \item private(list), shared(list)
    \item firstprivate(list), lastprivate(list)
    \item reduction(operator:list)
    \item schedule(method[,chunk\_size])
    \item nowait
    \item if(scalar\_expression)
    \item num\_thread(num)
    \item threadprivate(list), copyin(list)
    \item ordered
    \item more $\cdots$
  \end{itemize}
\end{frame}

\begin{frame}{\small Runtime Libraries}
  \begin{itemize}
    \item Number of Threads: omp\_\{set,get\}\_num\_threads
    \item Thread ID: omp\_get\_thread\_num
    \item Scheduling: omp\_\{set,get\}\_dynamic
    \item Nested Parallelism: omp\_in\_parallel
    \item Locking: omp\_\{init,set,unset\}\_lock
    \item Wallclock Timer: omp\_get\_wtime
    \item more $\cdots$
  \end{itemize}
\end{frame}

\begin{frame}{\small Environment Variables}
  \begin{itemize}
    \item OMP\_NUM\_THREADS
    \item OMP\_SCHEDULE
    \item OMP\_STACKSIZE
    \item OMP\_DYNAMIC
    \item OMP\_NESTED
    \item OMP\_WAIT\_POLICY
    \item more $\cdots$
  \end{itemize}
\end{frame}

\begin{frame}{\small Parallel Directive}
  \begin{itemize}
    \item The {\bf parallel} directive forms a team of threads for parallel execution.
    \item Each thread executes the block of code within the OpenMP Parallel region.
  \end{itemize}
  \begin{columns}
    \column{0.45\textwidth}
    \begin{eblock}{C}
      \lstinputlisting[language=OmpC]{openmp/hello/solution/helloworld.c}
    \end{eblock}
    \column{0.45\textwidth}
    \begin{eblock}{Fortran}
      \lstinputlisting[language={OmpFortran}]{openmp/hello/solution/helloworld.f90}
    \end{eblock}
  \end{columns}
\end{frame}
\scriptsize
\begin{frame}[fragile]
  \frametitle{\small Compilation and Execution}
  \begin{itemize}
    \item IBM Power7 clusters
    \begin{itemize}
      \item Use thread-safe compilers (with "\_r'')
      \item Use '-qsmp=omp' option
    \end{itemize}
    \item[] \% xlc\_r -qsmp=omp hello.c \&\& OMP\_NUM\_THREADS=4 ./a.out
    \item[] \% xlf90\_r -qsmp=omp hello.f90 \&\& OMP\_NUM\_THREADS=4 ./a.out
    \item Dell Linux clusters
    \begin{itemize}
      \item Use '-openmp' option (Intel compiler)
      \item Use '-fopenmp' option (GNU compiler)
      \item Use '-mp' option (PGI Compiler)
    \end{itemize}
    \item[] \% icc -openmp hello.c \&\& OMP\_NUM\_THREADS=4 ./a.out
    \item[] \% ifort -openmp hello.f90 \&\& OMP\_NUM\_THREADS=4 ./a.out
  \end{itemize}
    \begin{bblock}{}
      {\tiny
        \begin{verbatim}
altair:openmp apacheco$ gcc -fopenmp helloworld.c -o helloc.x
altair:openmp apacheco$ gfortran -fopenmp helloworld.f90 -o hellof90.x
altair:openmp apacheco$ OMP_NUM_THREADS=4 ./helloc.x 
Hello world
Hello world
Hello world
Hello world
altair:openmp apacheco$ OMP_NUM_THREADS=4 ./hellof90.x
Hello World
Hello World
Hello World
Hello World
        \end{verbatim}
      }
    \end{bblock}
\end{frame}

\begin{frame}
  \frametitle{\small Hello World: C}
  \begin{columns}
    \column{6cm}
    \tikzstyle{na} = [baseline=-.5ex]
    \begin{bblock}{}
       \begin{tabular}{lc}
        {\color{DarkGreen}\#include <omp.h>} \tikz[na] \node[coordinate] (n1) {}; & \\
        {\color{DarkGreen}\#include <stdio.h>} & \\
        int main () \{ & \\
        \quad {\color{blue}\#pragma omp parallel} & \\
        \quad \{ \tikz[na] \node[coordinate] (n2) {}; & \\
        \quad \quad printf("Hello from thread \%d out of \%d & \\
        \quad \quad\quad threads\textbackslash n'',{\color{red}omp\_get\_thread\_num()} \tikz[na] \node[coordinate] (n3) {}; , & \\
        \quad \quad\quad {\color{red}omp\_get\_num\_threads()}\tikz[na] \node[coordinate] (n4) {}; ); & \\
        \quad \} \tikz[na] \node[coordinate] (n5) {}; & \\
        \quad return 0; & \\
        \} & \\
      \end{tabular}
    \end{bblock}
    \column{4cm}
    \vspace{-0.5cm}
    \begin{itemize}
      \item[] \tikz[baseline]{\node[fill=blue!20,anchor=base] (t1) {OpenMP include file}; } 
      \item[]
      \item[] \tikz[baseline]{\node[fill=blue!20,anchor=base] (t2) {Parallel region starts here};  } 
      \item[]
      \item[] \tikz[baseline]{\node[fill=blue!20,anchor=base] (t3) {Runtime library functions};  }
      \item[]
      \item[] \tikz[baseline]{\node[fill=blue!20,anchor=base] (t4) {Parallel region ends here};  }
    \end{itemize}
    \begin{tikzpicture}[overlay]
        \path[->] (t1) edge (n1);
        \path[->] (t2) edge (n2);
        \path[->] (t3) edge (n3);
        \path[->] (t3) edge (n4);
        \path[->] (t4) edge (n5);
    \end{tikzpicture}
  \end{columns}
  \begin{columns}
    \column{5cm}
    \begin{eblock}{Output}
      Hello from thread 0 out of 4 threads\\
      Hello from thread 1 out of 4 threads\\
      Hello from thread 2 out of 4 threads\\
      Hello from thread 3 out of 4 threads
    \end{eblock}
  \end{columns}
\end{frame}

\begin{frame}
  \frametitle{\small Hello World: Fortran}
  \begin{columns}
    \column{8cm}
    \tikzstyle{na} = [baseline=-.5ex]
    \begin{bblock}{}
       \begin{tabular}{lc}
         program hello & \\
         \\
        \quad implicit none & \\
        \quad integer :: omp\_get\_thread\_num, omp\_get\_num\_threads & \\
        \\
        \quad {\color{blue} !\$omp parallel} \tikz[na] \node[coordinate] (n2) {}; & \\
        \\
        \quad print *, 'Hello from thread',{\color{red}omp\_get\_thread\_num()} \tikz[na] \node[coordinate] (n3) {}; , \& & \\
        \quad\quad 'out of ' {\color{red}omp\_get\_num\_threads()}\tikz[na] \node[coordinate] (n4) {};,' threads' & \\
        \\
        \quad {\color{blue} !\$omp end parallel} \tikz[na] \node[coordinate] (n5) {}; & \\
        end program hello & \\ 
      \end{tabular}
    \end{bblock}
    \column{4cm}
%    \vspace{-0.5cm}
    \begin{itemize}
 %     \item[] \tikz[baseline]{\node[fill=blue!20,anchor=base] (t1) {OpenMP include file}; } 
 %     \item[]
      \item[]
      \item[]
      \item[] \tikz[baseline]{\node[fill=blue!20,anchor=base] (t2) {Parallel region starts here};  } 
      \item[]
      \item[] \tikz[baseline]{\node[fill=blue!20,anchor=base] (t3) {Runtime library functions};  }
      \item[]
      \item[] \tikz[baseline]{\node[fill=blue!20,anchor=base] (t4) {Parallel region ends here};  }
    \end{itemize}
    \begin{tikzpicture}[overlay]
%        \path[->] (t1) edge (n1);
        \path[->] (t2) edge  (n2);
        \path[->] (t3) edge [bend right] (n3);
        \path[->] (t3) edge [bend left] (n4);
        \path[->] (t4) edge [bend left] (n5);
    \end{tikzpicture}
  \end{columns}
  \vspace{0.8cm}
  \begin{columns}
    \column{5cm}
    \begin{eblock}{Output}
      Hello from thread 0 out of 4 threads\\
      Hello from thread 1 out of 4 threads\\
      Hello from thread 2 out of 4 threads\\
      Hello from thread 3 out of 4 threads
    \end{eblock}
  \end{columns}
\end{frame}

\footnotesize

\begin{frame}[fragile]
  \frametitle{\small Exercise 1: Hello World}
  \begin{itemize}
    \item Write a ``hello world'' program with OpenMP where
      \begin{enumerate}
        \item If the thread id is odd, then print a message "Hello world from thread x, I'm odd!''
        \item If the thread id is even, then print a message "Hello world from thread x, I'm even!''
      \end{enumerate}
  \end{itemize}
  \begin{columns}
    \column{0.5\textwidth}
    \begin{bblock}{C}
      \lstinputlisting[language=OmpC]{openmp/hello/exercise/hello.c}
    \end{bblock}
    \column{0.5\textwidth}
    \begin{bblock}{Fortran}
      \lstinputlisting[language=OmpFortran]{openmp/hello/exercise/hello.f90}
    \end{bblock}
  \end{columns}
\end{frame}

\begin{frame}[fragile]
  \frametitle{\small Solution}
  \begin{columns}
    \column{0.5\textwidth}
    \begin{eblock}{C/C++}
      \lstinputlisting[language=OmpC]{openmp/hello/solution/hello.c}
    \end{eblock}
    \begin{bblock}{}
      {\fontsize{4}{5}\selectfont
        \begin{Verbatim}
altair:solution apacheco$ gcc -fopenmp -o helloc hello.c
altair:solution apacheco$ ./helloc
Hello world from thread 1, I am odd
Hello world from thread 2, I am even
Hello world from thread 0, I am even
Hello world from thread 3, I am odd
        \end{Verbatim}
      }
    \end{bblock}
    \column{0.5\textwidth}
    \begin{eblock}{Fortran}
      \lstinputlisting[language={OmpFortran}]{openmp/hello/solution/hello.f90}
    \end{eblock}
    \begin{bblock}{}
      {\fontsize{4}{5}\selectfont
        \begin{Verbatim}
altair:solution apacheco$ gfortran -fopenmp -o hellof hello.f90
altair:solution apacheco$ ./hellof
 Hello from thread           2 , I am even!
 Hello from thread           1 , I am odd!
 Hello from thread           0 , I am even!
 Hello from thread           3 , I am odd!
        \end{Verbatim}
      }
    \end{bblock}
  \end{columns}
\end{frame}

\begin{frame}
  \frametitle{\small Work Sharing: Parallel Loops}
  \begin{itemize}
    \item We need to share work among threads to achieve parallelism
    \item Syntax:
    \begin{itemize}
      \item Fortran: \texttt{!\$omp parallel}
      \item C/C++: \texttt{\#pragma for}
    \end{itemize}
    \item Loops are the most likely targets when parallelizing a serial program
    \item Syntax:
    \begin{itemize}
      \item Fortran: \texttt{!\$omp do}
      \item C/C++: \texttt{\#pragma omp for}
    \end{itemize}
    \item Other work sharing directives available
    \begin{itemize}
      \item Sections: \texttt{!\$omp sections} or \texttt{\#pragma sections} 
      \item Tasks: \texttt{!\$omp task} or \texttt{\#pragma omp task}
    \end{itemize}
    \item The parallel and work sharing directive can be combined as
    \begin{itemize}
      \item \texttt{!\$omp parallel do}
      \item \texttt{\#pragma omp parallel sections}
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{\small Example: Parallel Loops}
  \begin{columns}
    \column{0.45\textwidth}
    \begin{eblock}{C/C++}
        \begin{lstlisting}[language=OmpC]
#include <omp.h>

int main() {
  int i = 0, n = 100, a[100];
  #pragma omp parallel for
  for (i = 0; i < n ; i++) {
    a[i] = (i+1) * (i+2) ;
  }
}
        \end{lstlisting}
     \end{eblock}
    \column{0.45\textwidth}
    \begin{eblock}{Fortran}
        \begin{lstlisting}[language=OmpFortran]
program paralleldo

  implicit none
  integer :: i, n, a(100)
  
  i = 0
  n = 100
  !$omp parallel 
  !$omp do
  do i = 1, n
     a(i) = i * (i+1)
  end do
  !$omp end do
  !$omp end parallel
end program paralleldo
        \end{lstlisting}
    \end{eblock}
  \end{columns}
\end{frame}

\begin{frame}[allowframebreaks]
  \frametitle{\small Load Balancing}
  \begin{itemize}
    \item OpenMP provides different methods to divide iterations among threads, indicated by the \texttt{schedule} clause
    \begin{itemize}
      \item Syntax: \texttt{schedule (<method>, [chunk size])}
    \end{itemize}
    \item Methods include
    \begin{itemize}
      \item \texttt{Static}: the default schedule; divide interations into chunks according to \texttt{size}, then distribute chunks to each thread in a round-robin manner.
      \item \texttt{Dynamic}: each thread grabs a chunk of iterations, then requests another chunk upon completion of the current one, until all iterations are executed.
      \item \texttt{Guided}: similar to \texttt{Dynamic}; the only difference is that the chunk size starts large and shrinks to \texttt{size} eventually.
    \end{itemize}
  \end{itemize}

  \begin{bblock}{4 threads, 100 iterations}
    \begin{tabular}{ccccc}
      \multirow{2}{*}{Schedule}& \multicolumn{4}{c}{Iterations mapped onto thread}\\
      & 0 & 1 & 2 & 3 \\
      \hline
      \texttt{Static} & 1-25 & 26-50 & 51-75 & 76-100 \\
      \texttt{Static,20} & 1-20, 81-100 & 21-40 & 41-60 & 61-80 \\
      \texttt{Dynamic} & $1,\cdots$ & $2,\cdots$ & $3,\cdots$ & $4,\cdots$ \\
      \texttt{Dynamic,10} & $1-10,\cdots$ & $11-20,\cdots$ & $21-30,\cdots$ & $31-40,\cdots$ \\
      \hline\\
    \end{tabular}
  \end{bblock}

  \begin{bblock}{}
    \begin{center}
      \begin{tabular}{cl}
        Schedule & When to Use \\
        \hline\\
        \multirow{3}{*}{\texttt{Static}} & Even and predictable workload per iteration; \\
        & scheduling may be done at compilation time,\\ 
        & least work at runtime.\\
        \\
        \multirow{2}{*}{\texttt{Dynamic}} & Highly variable and unpredictable workload \\
        & per iteration; most work at runtime \\
        \\
        \multirow{3}{*}{\texttt{Guided}} & Special case of \texttt{dynamic} scheduling; \\
        & compromise between load balancing and \\
        & scheduling overhead at runtime \\
      \end{tabular}
    \end{center}
  \end{bblock}
\end{frame}

\begin{frame}[fragile]
  \frametitle{\small Work Sharing: Sections}
  \begin{itemize}
    \item Gives a different block to each thread
  \end{itemize}
  \begin{columns}
    \column{0.55\textwidth}
    \begin{eblock}{C/C++}
      \lstinputlisting[language=OmpC,firstline=10,lastline=21]{./openmp/tmp.c}
    \end{eblock}
    \column{0.5\textwidth}
    \begin{eblock}{Fortran}
      \lstinputlisting[language=OmpFortran,firstline=14,lastline=23]{./openmp/tmp.f90}
    \end{eblock}
  \end{columns}
\end{frame}


\begin{frame}
  \frametitle{\small Scope of variables}
  \begin{itemize}
    \item \texttt{Shared(list)}
    \begin{itemize}
      \item Specifies the variables that are shared among all threads
    \end{itemize}
    \item \texttt{Private(list)}
    \begin{itemize}
      \item Creates a local copy of the specified variables for each thread
      \item the value is uninitialized!
    \end{itemize}
    \item \texttt{Default(shared|private|none)}
    \begin{itemize}
      \item Defines the default scope of variables
      \item \textbf{C/C++ API does not have \texttt{default(private)}}
    \end{itemize}
    \item Most variables are shared by default
    \begin{itemize}
      \item A few exceptions: iteration variables; stack variables in subroutines; automatic variables within a statement block.
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{\small Exercise: SAXPY}
  \begin{itemize}
    \item SAXPY is a common operation in computations with vector processors included as part of the BLAS routines
    \item[] $y\leftarrow \alpha x + y$
    \item SAXPY is a combination of scalar multiplication and vector addition
    \item Parallelize the following SAXPY code
  \end{itemize}
  \begin{columns}
    \column{0.5\textwidth}
    \begin{bblock}{C}
      \lstinputlisting[basicstyle=\fontsize{3.5}{4}\selectfont\ttfamily,language=OmpC]{openmp/saxpy/exercise/saxpy.c}
    \end{bblock}
    \column{0.5\textwidth}
    \begin{bblock}{Fortran}
      \lstinputlisting[basicstyle=\fontsize{3.5}{4}\selectfont\ttfamily,language=OmpFortran]{openmp/saxpy/exercise/saxpy.f90}
    \end{bblock}
  \end{columns}
\end{frame}

\begin{frame}{\small Solution: SAXPY}
  \begin{columns}
    \column{0.5\textwidth}
    \begin{eblock}{C}
      \lstinputlisting[basicstyle=\fontsize{3.5}{4}\ttfamily,language=OmpC]{openmp/saxpy/solution/saxpy_omp.c}
    \end{eblock}
    \column{0.5\textwidth}
    \begin{eblock}{Fortran}
      \lstinputlisting[basicstyle=\fontsize{3.5}{4}\selectfont\ttfamily,language=OmpFortran]{openmp/saxpy/solution/saxpy_omp.f90}
    \end{eblock}
  \end{columns}
  \begin{columns}
    \column{0.8\textwidth}
    {
      \begin{eblock}{}
        \begin{tabular}{|c|c|c|c|}
          \hline
          Language & Serial & OpenMP (16 Threads) & SpeedUp \\
          \hline
          C & 0.511 & 0.186 & 2.75 \\ \hline
          Fortran & 0.993 & 0.244 & 4.07 \\
          \hline
        \end{tabular}
      \end{eblock}
    }
  \end{columns}
\end{frame}

\begin{frame}[allowframebreaks]{\small Exercise: Matrix Multiplication}
  \begin{itemize}
    \item Most Computational code involve matrix operations such as matrix multiplication.
    \item Consider a matrix {\bf C} of two matrices {\bf A} and {\bf B}:
    \item[] Element {\it i,j} of {\bf C} is the dot product of the $i^{th}$ row of {\bf A} and $j^{th}$ column of {\bf B}
  \end{itemize}
  \begin{center}
    \includegraphics[width=0.5\textwidth]{./matmul}
  \end{center}
  \begin{itemize}
    \item Parallelize the following MATMUL code
  \end{itemize}
  \begin{columns}
    \column{0.5\textwidth}
    \begin{bblock}{C}
      \lstinputlisting[basicstyle=\fontsize{3.5}{3.5}\selectfont\ttfamily,language=OmpC]{openmp/matmul/exercise/matmul.c}
    \end{bblock}
    \column{0.5\textwidth}
    \begin{bblock}{Fortran}
      \lstinputlisting[basicstyle=\fontsize{3.5}{3.5}\selectfont\ttfamily,language=OmpFortran]{openmp/matmul/exercise/matmul.f90}
    \end{bblock}
  \end{columns}
\end{frame}

\begin{frame}{\small Solution: MATMUL}
  \begin{columns}
    \column{0.5\textwidth}
    \begin{eblock}{C}
      \lstinputlisting[basicstyle=\fontsize{3.5}{3.5}\selectfont\ttfamily,language=OmpC]{openmp/matmul/solution/matmul_omp.c}
    \end{eblock}
    \column{0.5\textwidth}
    \begin{eblock}{Fortran}
      \lstinputlisting[basicstyle=\fontsize{3.5}{3.5}\selectfont\ttfamily,language=OmpFortran]{openmp/matmul/solution/matmul_omp.f90}
    \end{eblock}
  \end{columns}
\end{frame}

\begin{frame}[fragile]
  \frametitle{\small Pitfalls: False Sharing}
  \begin{itemize}
    \item Array elements that are in the same cache line can lead to false sharing.
    \begin{itemize}
      \item The system handles cache coherence on a cache line basis, not on a byte or word basis.
      \item Each update of a single element could invalidate the entire cache line.
    \end{itemize}
  \end{itemize}
  \begin{columns}
    \column{0.6\textwidth}
    \begin{eblock}{}
      \lstinputlisting[language=OmpFortran,firstline=51,lastline=57]{./openmp/tmp.f90}
    \end{eblock}
  \end{columns}
\end{frame}

\begin{frame}
  \frametitle{\small Pitfalls: Race Condition}
  \begin{itemize}
    \item Multiple threads try to write to the same memory location at the same time.
    \begin{itemize}
      \item Indeterministic results
    \end{itemize}
    \item Inappropriate scope of varibale can cause indeterministic results too.
    \item When having indeterministic results, set the number of threads to 1 to check
    \begin{itemize}
      \item If problem persists: scope problem
      \item If problem is solved: race condition
    \end{itemize}
  \end{itemize}
  \begin{columns}
    \column{0.5\textwidth}
    \begin{eblock}{}
      \lstinputlisting[language=OmpFortran,firstline=59,lastline=65]{./openmp/tmp.f90}
    \end{eblock}
  \end{columns}
\end{frame}

\begin{frame}
  \frametitle{\small Synchronization: Barrier}
  \begin{itemize}
    \item ``Stop sign'' where every thread waits until all threads arrive.
    \item Purpose: protect access to shared data.
    \item Syntax:
    \begin{itemize}
      \item Fortran: \texttt{!\$omp barrier}
      \item C/C++: \texttt{\#pragma omp barrier}
    \end{itemize}
    \item A barrier is implied at the end of every parallel region
    \begin{itemize}
      \item Use the \texttt{nowait} clause to turn it off
    \end{itemize}
    \item Synchronizations are costly so their usage should be minimized.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{\small Synchronization: Crtitical and Atomic}
  \begin{itemize}
    \item Critical: Only one thread at a time can enter a \texttt{critical} region 
  \end{itemize}
  \begin{columns}
    \column{0.5\textwidth}
    \begin{eblock}{}
      \lstinputlisting[language=OmpFortran,firstline=67,lastline=73]{./openmp/tmp.f90}
    \end{eblock}
  \end{columns}
  \begin{itemize}
    \item Atomic: Only one thread at a time can update a memory location 
  \end{itemize}
  \begin{columns}
    \column{0.5\textwidth}
    \begin{eblock}{}
      \lstinputlisting[language=OmpFortran,firstline=75,lastline=81]{./openmp/tmp.f90}
    \end{eblock}  
  \end{columns}
\end{frame}

\begin{frame}[fragile]
  \frametitle{\small Private Variables}
  \begin{itemize}
    \item Not initialized at the beginning of parallel region.
    \item After parallel region
    \begin{itemize}
      \item Not defined in OpenMP 2.x
      \item 0 in OpenMP 3.x
    \end{itemize}
  \end{itemize}
  \def\firstnode{\tikz[remember picture,baseline=-0.5mm] \node (n1) {tmp};}  
  \def\secondnode{\tikz[remember picture,baseline=-0.5mm] \node (n2) {tmp};} 
  \begin{eblock}{}
    \centering{\hspace{1.45cm}\tikz[remember picture] \node[coordinate,fill=blue!20,rectangle] (t1) {tmp not initialized here};}
    {\scriptsize
      \begin{Verbatim} [commandchars=\|\§\! ]
        void wrong()
        {
          int tmp = 0;
          #pragma omp for private(|firstnode)
          for (int j = 0; j < 100; ++j)
          tmp += j
          printf("%d\n", |secondnode)
        }
      \end{Verbatim}
    }
    \vspace{0.2cm}
    {\hspace{-2.3cm}
      \tikz[remember picture] \node[coordinate,fill=blue!20,rectangle] (t2) {OpenMP 2.5: tmp undefined};
      \tikz[remember picture] \node[coordinate,fill=blue!20,rectangle] (t3) {OpenMP 3.0: tmp is 0};
}
    \begin{tikzpicture}[remember picture,overlay]
      \path[->] (t1) edge (n1);
      \path[->] (t2) edge (n2);
      \path[->] (t3) edge (n2);
    \end{tikzpicture}
  \end{eblock}
\end{frame}

\begin{frame}[fragile]
  \frametitle{\small Special Cases of Private}
  \begin{itemize}
    \item Firstprivate
    \begin{itemize}
      \item Initialize each private copy with the corresponding value from the master thread
    \end{itemize}
    \item Lastprivate
    \begin{itemize}
      \item Allows the value of a private variable to be passed to the shared variable outside the parallel region
    \end{itemize}
  \end{itemize}
  \def\firstnode{\tikz[remember picture,baseline=-0.5mm] \node (n1) {tmp};}  
  \def\secondnode{\tikz[remember picture,baseline=-0.5mm] \node (n2) {tmp};} 
  \begin{eblock}{}
    \centering{\hspace{0.3cm}\tikz[remember picture] \node[coordinate,fill=blue!20,rectangle] (t1) {tmp initialized as 0};}
    {\scriptsize
      \begin{Verbatim} [commandchars=\|\§\! ]
void wrong()
{
  int tmp = 0;
  #pragma omp for firstprivate(|firstnode) lastprivate(tmp)
  for (int j = 0; j < 100; ++j)
    tmp += j
  printf("%d\n", |secondnode)
}
      \end{Verbatim}
    }
    \vspace{0.2cm}
    {\hspace{-4.3cm}\tikz[remember picture] \node[coordinate,fill=blue!20,rectangle] (t2) {The value of tmp is the value when j=99};}
    \begin{tikzpicture}[remember picture,overlay]
      \path[->] (t1) edge (n1);
      \path[->] (t2) edge (n2);
    \end{tikzpicture}
  \end{eblock}
\end{frame}

\begin{frame}
  \frametitle{\small Exercise 2: Calculate pi by Numerical Integration}
  \begin{columns}
    \column{5cm}
    \begin{itemize}
      \item We know that
      \begin{align*}
        \int^1_0 \dfrac{4.0}{(1+x^2)}\, dx = \pi
      \end{align*}
      \item So numerically, we can approxiate pi as the sum of a number of rectangles
      \begin{align*}
        \sum^N_{i=0}\,F(x_i)\Delta x \approx \pi
      \end{align*}
      \item[] \fontsize{4}{5}{ Meadows et al, A ``hands-on'' introduction to OpenMP, SC09 }
    \end{itemize}
    \column{5cm}
    \begin{center}
      \includegraphics[width=4cm]{./pi}
    \end{center}
  \end{columns}
\end{frame}

\begin{frame}[fragile]
  \frametitle{\small Exercise 2: Rewrite for OpenMP parallelization}
  \begin{columns}
    \column{0.5\textwidth}
    \begin{eblock}{C/C++}
      \lstinputlisting[basicstyle=\fontsize{5}{6}\selectfont\ttfamily,language=C]{./openmp/calc_pi/exercise/pi_serial.c}
    \end{eblock}
    \column{0.5\textwidth}
    \begin{eblock}{Fortran}
      \lstinputlisting[basicstyle=\fontsize{5}{6}\selectfont\ttfamily,language={[90]Fortran}]{./openmp/calc_pi/exercise/pi_serial.f90}
    \end{eblock}
  \end{columns}
\end{frame}

\begin{frame}[fragile,allowframebreaks]
  \frametitle{\small Solution (Very Slow)}
  \begin{columns}
    \column{0.5\textwidth}
    \begin{eblock}{C/C++}
      \lstinputlisting[basicstyle=\fontsize{5}{6}\selectfont\ttfamily,language=OmpC]{./openmp/calc_pi/solution/pi_omp1.c}
    \end{eblock}
    \column{0.5\textwidth}
    \begin{eblock}{Fortran}
      \lstinputlisting[basicstyle=\fontsize{5}{6}\ttfamily,language=OmpFortran]{./openmp/calc_pi/solution/pi_omp1.f90}
    \end{eblock}
  \end{columns}
  \begin{bblock}{}
    {\tiny
      \begin{verbatim}
altair:openmp apacheco$ gcc pi_serial.c -o pic
altair:openmp apacheco$ gcc -fopenmp pi_omp1.c -o pic_omp
altair:openmp apacheco$ gfortran pi_serial.f90 -o pif
altair:openmp apacheco$ gfortran -fopenmp pi_omp1.f90 -o pif_omp
altair:solution apacheco$ echo ``Serial C Code''; ./pic
Serial C Code
pi = 3.141592653590426
time to compute = 1.72441 seconds
altair:solution apacheco$ echo ``OMP C Code with Atomic''; ./pic_omp
OMP C Code with Atomic
pi = 3.141592653590195
time to compute = 6.10142 seconds
altair:solution apacheco$ echo ``Serial F90 Code''; ./pif
Serial F90 Code
pi = 3.141592673590427
time to compute = 0.988196 seconds
altair:solution apacheco$ echo ``OMP F90 Code with Atomic''; ./pif_omp
OMP F90 Code with Atomic
pi = 3.141592673590174
time to compute = 7.368610 seconds
      \end{verbatim}
    }
  \end{bblock}
  \begin{itemize}
    \item What is the value of pi if you did not have the \textit{atomic} directive?
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{\small Reduction}
  \begin{itemize}
    \item The \texttt{reduction} clause allows accumulative operations on the value of variables.
    \item Syntax: \texttt{reduction (operator:variable list)}
    \item A private copy of each variable which appears in \texttt{reduction} is created as if the \texttt{private} clause is specified.
    \item Operators
    \begin{enumerate}
      \item Arithmetic
      \item Bitwise
      \item Logical
    \end{enumerate}
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{\small Example: Reduction}
  \begin{columns}
    \column{0.5\textwidth}
    \begin{eblock}{C/C++}
      {\tiny
        \begin{lstlisting}[basicstyle=\tiny\ttfamily,language=OmpC]
#include <omp.h>
int main() {
  int i, n = 100, sum , a[100], b[100];
  for (i = 0; i < n; i++) {
    a[i] = i;
    b[i] = 1;
  }
  sum = 0;
#pragma omp parallel for reduction(+:sum)
  for (i = 0; i < n ; i++) {
    sum += a[i] * b[i];
  }
}
        \end{lstlisting}
      }
    \end{eblock}
    \column{0.45\textwidth}
    \begin{eblock}{Fortran}
      {\tiny
        \begin{lstlisting}[basicstyle=\tiny\ttfamily,language=OmpFortran]
program reduction
  
  implicit none
  integer :: i, n, sum , a(100), b(100)

  n = 100 ; b = 1; sum = 0
  do i = 1 , n
     a(i) = i
  end do
  !$omp parallel do reduction(+:sum)
  do i = 1, n
     sum = sum + a(i) * b(i)
  end do
  !$omp end parallel do

end program reduction
        \end{lstlisting}
      }
    \end{eblock}
  \end{columns}
\end{frame}

\begin{frame}
  \frametitle{\small Exercise 3: pi calculation with reduction}
  \begin{itemize}
    \item Redo exercise 2 with reduction
  \end{itemize}
\end{frame}

\begin{frame}[fragile,allowframebreaks]
  \frametitle{\small Solution: pi calculation with reduction}
  \begin{columns}
    \column{5cm}
    \begin{eblock}{C }
      \lstinputlisting[basicstyle=\fontsize{5}{6}\selectfont\ttfamily,language=OmpC]{./openmp/calc_pi/solution/pi_omp.c}
    \end{eblock}
    \column{5cm}
    \begin{eblock}{Fortran}
      \lstinputlisting[basicstyle=\fontsize{5}{6}\ttfamily,language=OmpFortran]{./openmp/calc_pi/solution/pi_omp.f90}
    \end{eblock}
  \end{columns}
  \begin{bblock}{}
    {\tiny
      \begin{verbatim}
altair:openmp apacheco$ gcc -fopenmp pi_omp.c -o pic_ompr
altair:openmp apacheco$ gfortran -fopenmp pi_omp.f90 -o pif_ompr
altair:solution apacheco$ echo ``Serial C Code''; ./pic
Serial C Code
pi = 3.141592653590426
time to compute = 1.72441 seconds
altair:solution apacheco$ echo ``OMP C Code with Atomic''; ./pic_omp
OMP C Code with Atomic
pi = 3.141592653590195
time to compute = 6.10142 seconds
altair:solution apacheco$ echo ``OMP C Code with Reduction''; ./pic_ompr
OMP C Code with Reduction
pi = 3.141592653589683
time to compute = 0.48712 seconds
altair:solution apacheco$ echo ``Serial F90 Code''; ./pif
Serial F90 Code
pi = 3.141592673590427
time to compute = 0.988196 seconds
altair:solution apacheco$ echo ``OMP F90 Code with Atomic''; ./pif_omp
OMP F90 Code with Atomic
pi = 3.141592673590174
time to compute = 7.368610 seconds
altair:solution apacheco$ echo ``OMP F90 Code with Reduction''; ./pif_ompr
OMP F90 Code with Reduction
pi = 3.141592673589683
time to compute = 0.400939 seconds
      \end{verbatim}
    }
  \end{bblock}
\end{frame}

\begin{frame}
  \frametitle{\small Runtime Library Functions}
  \begin{itemize}
    \item Modify/query the number of threads
    \begin{itemize}
      \item \texttt{omp\_set\_num\_threads()}, \texttt{omp\_get\_num\_threads()}, \texttt{omp\_get\_thread\_num()}, \texttt{omp\_get\_max\_threads()}
    \end{itemize}
    \item Query the number of processors
    \begin{itemize}
      \item \texttt{omp\_num\_procs()}
    \end{itemize}
    \item Query whether or not you are in an active parallel region
    \begin{itemize}
      \item \texttt{omp\_in\_parallel()}
    \end{itemize}
    \item Control the behavior of dynamic threads
    \begin{itemize}
      \item \texttt{omp\_set\_dynamic(),omp\_get\_dynamic()}
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{\small Environment Variables}
  \begin{itemize}
    \item OMP\_NUM\_THREADS: set default number of threads to use.
    \item OMP\_SCHEDULE: control how iterations are scheduled for parallel loops.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{\small References}
  \begin{itemize}
    \item \url{https://docs.loni.org/wiki/Using_OpenMP}
    \item \url{http://en.wikipedia.org/wiki/OpenMP}
    \item \url{http://www.nersc.gov/nusers/help/tutorials/openmp}
    \item \url{http://www.llnl.gov/computing/tutorials/openMP}
    \item \url{http://www.citutor.org}
  \end{itemize}
\end{frame}

\end{document}

